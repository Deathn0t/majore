{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pprint import pprint, pformat\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "from load_audio import  AudioFeatureDataset\n",
    "from load_text import TextDataset\n",
    "from load_video import VideoDataset\n",
    "from load_multimodal_data import MultimodalDataset\n",
    "\n",
    "from position_encoder import PositionalEncoding\n",
    "\n",
    "from encoders import AudioEncoder, VideoEncoder, DotProductAttention\n",
    "\n",
    "from decoder import MultimodalDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_how2 = \"/Volumes/LaCie/vision/data/\"\n",
    "\n",
    "video_path = os.path.join(path_how2, \"resnext101-action-avgpool-300h\", \"train.npy\")\n",
    "\n",
    "texts_path = os.path.join(path_how2,\"how2-300h-v1/data/train\", \"text.en\")\n",
    "embeddings_path = os.path.join(path_how2, \"how2-release/word_embedding/\",\"cmu_partition.train.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/LaCie/vision/majore/scripts/data/load_text.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return  np.array(text), np.array(splitted), largest_split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184949 184949 184949\n"
     ]
    }
   ],
   "source": [
    "video_dataset = VideoDataset(video_path)\n",
    "audio_dataset = AudioFeatureDataset(path_how2,\"train\")\n",
    "text_dataset = TextDataset(texts_path, embeddings_path)\n",
    "\n",
    "print(len(video_dataset),len(audio_dataset),len(text_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_dataset = MultimodalDataset(video_dataset, audio_dataset, text_dataset)\n",
    "dataloader = DataLoader(multimodal_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2048])\n",
      "torch.Size([4, 10807, 43])\n",
      "torch.Size([4, 225, 100])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    \n",
    "    print(batch[\"video\"][\"video\"].shape)\n",
    "    print(batch[\"audio\"].shape)\n",
    "    print(batch[\"text\"][\"embedding\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 480\n",
    "d_feedforward = 1920\n",
    "dropout = 0.2\n",
    "nhead = 6\n",
    "nlayer_audio = 6\n",
    "nlayer_video = 1\n",
    "\n",
    "video_dim = 2048\n",
    "audio_size = 10807\n",
    "audio_dim = 43\n",
    "\n",
    "audio_encoder = AudioEncoder(audio_dim,\n",
    "                             audio_size,\n",
    "                             nhead,\n",
    "                             nlayer_audio,\n",
    "                             d_model,\n",
    "                             d_feedforward,\n",
    "                             dropout)\n",
    "\n",
    "video_encoder = VideoEncoder(video_dim,\n",
    "                             nhead,\n",
    "                             nlayer_video,\n",
    "                             d_model,\n",
    "                             d_feedforward,\n",
    "                             dropout)\n",
    "\n",
    "dot_product_attention = DotProductAttention(d_model, 480, 480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = next(iter(dataloader))\n",
    "audio = batch1[\"audio\"].float()\n",
    "video = batch1[\"video\"][\"video\"].unsqueeze(1).float()\n",
    "text = batch1[\"text\"][\"embedding\"].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10807, 480])\n",
      "torch.Size([4, 1, 480])\n",
      "torch.Size([4, 10807, 480])\n"
     ]
    }
   ],
   "source": [
    "out_audio = audio_encoder(audio)\n",
    "out_video = video_encoder(video)\n",
    "output_encoder = dot_product_attention.forward(out_audio, out_video, out_video)\n",
    "\n",
    "print(out_audio.shape)\n",
    "print(out_video.shape)\n",
    "print(output_encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 225, 480])\n",
      "torch.Size([4, 10807, 480])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/LaCie/vision/majore/scripts/data/decoder.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc2(x))\n"
     ]
    }
   ],
   "source": [
    "text_dim = 100\n",
    "text_size = 225\n",
    "vocab_size = 10807\n",
    "n_layer = 4\n",
    "d_model = 480\n",
    "d_feedforward = 1920\n",
    "dropout = 0.2\n",
    "nhead = 6\n",
    "\n",
    "decoder = MultimodalDecoder(text_dim, text_size, vocab_size, nhead,  n_layer, d_model, d_feedforward, dropout)\n",
    "decoded = decoder(text, output_encoder)\n",
    "print(decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
